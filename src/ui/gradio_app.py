"""
Gradio UIå®Ÿè£…
"""
import gradio as gr
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple

from src.core.image_parser import ImageParser
from src.core.model_manager import ModelManager
from src.core.vlm_interface import VLMInterface
from src.utils.config_loader import ConfigLoader

# GGUFå¯¾å¿œï¼ˆllama-cpp-pythonãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ï¼‰
try:
    from src.core.vlm_interface_gguf import VLMInterfaceGGUF
    GGUF_AVAILABLE = True
except ImportError:
    GGUF_AVAILABLE = False
    print("Warning: llama-cpp-python is not installed. GGUF models will not be available.")


class PromptAnalyzerUI:
    """ãƒ¡ã‚¤ãƒ³UIã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: Dict):
        """
        Args:
            config: settings.yamlã‹ã‚‰èª­ã¿è¾¼ã‚“ã è¨­å®š
        """
        self.config = config
        self.model_manager = ModelManager(config['paths']['models_dir'])
        self.current_vlm: Optional[VLMInterface] = None
        self.current_image_path: Optional[str] = None
        self.current_metadata: Optional[Dict] = None
        self.selected_model_path: Optional[str] = None  # é¸æŠã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        self.last_model_cache_file = Path(".last_model_cache.json")

        # ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
        config_loader = ConfigLoader()
        self.model_presets = config_loader.load_model_presets()

    def create_interface(self) -> gr.Blocks:
        """
        Gradio UIã‚’æ§‹ç¯‰

        UIæ§‹æˆ:
        - ã‚¿ãƒ–1: ç”»åƒåˆ†æ
        - ã‚¿ãƒ–2: ãƒ¢ãƒ‡ãƒ«ç®¡ç†
        - ã‚¿ãƒ–3: è¨­å®š
        """
        # ã‚«ã‚¹ã‚¿ãƒ CSSï¼ˆãƒ•ã‚©ãƒ³ãƒˆå¤‰æ›´ï¼‰
        custom_css = """
        * {
            font-family: "Segoe UI", "Yu Gothic", "Meiryo", Arial, sans-serif !important;
        }
        """

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ¨è«–è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆãªã‘ã‚Œã°configã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
        cached_settings = self.load_inference_settings()
        initial_temperature = cached_settings.get('temperature', self.config['inference']['temperature'])
        initial_max_tokens = cached_settings.get('max_tokens', self.config['inference']['max_tokens'])
        initial_top_p = cached_settings.get('top_p', self.config['inference']['top_p'])

        with gr.Blocks(title="SD Prompt Analyzer", css=custom_css) as interface:
            gr.Markdown("# SD Prompt Analyzer")
            gr.Markdown("Stable Diffusionç”»åƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ†æã™ã‚‹ãƒ„ãƒ¼ãƒ«")

            with gr.Tabs():
                # ã‚¿ãƒ–1: ç”»åƒåˆ†æ
                with gr.Tab("ç”»åƒåˆ†æ"):
                    with gr.Row():
                        # å·¦å´: ç”»åƒè¡¨ç¤º
                        with gr.Column(scale=1):
                            image_display = gr.Image(
                                label="ã“ã“ã«ç”»åƒã‚’ãƒ‰ãƒ­ãƒƒãƒ—",
                                type="filepath",
                                sources=["upload"],
                                height=400
                            )

                            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±è¡¨ç¤º
                            with gr.Accordion("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±", open=True):
                                prompt_display = gr.Textbox(
                                    label="Prompt",
                                    lines=3,
                                    interactive=False
                                )
                                negative_prompt_display = gr.Textbox(
                                    label="Negative Prompt",
                                    lines=2,
                                    interactive=False
                                )
                                settings_display = gr.Code(
                                    label="Settings",
                                    language="json",
                                    interactive=False,
                                    lines=5
                                )

                        # å³å´: ãƒãƒ£ãƒƒãƒˆ
                        with gr.Column(scale=1):
                            chatbot = gr.Chatbot(label="AIåˆ†æ", height=500)
                            clear_btn = gr.Button("ğŸ—‘ï¸ ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚¯ãƒªã‚¢", size="sm", variant="secondary")
                            context_info = gr.Markdown(
                                value="<small style='color: gray;'>--</small>",
                                elem_id="context-info"
                            )

                            # è³ªå•ãƒ—ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
                            gr.Markdown("### ã‚¯ã‚¤ãƒƒã‚¯è³ªå•")
                            with gr.Row():
                                preset_btn_1 = gr.Button("ğŸ“¸ ã“ã®ç”»åƒã«ã¤ã„ã¦èª¬æ˜", size="sm")
                                preset_btn_2 = gr.Button("âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã®ä¸€è‡´ç¢ºèª", size="sm")
                            with gr.Row():
                                preset_btn_3 = gr.Button("âœ¨ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹å–„æ¡ˆ", size="sm")
                                preset_btn_4 = gr.Button("ğŸ“ è©³ç´°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆææ¡ˆ", size="sm")

                            user_input = gr.Textbox(
                                label="è³ªå•ã‚’å…¥åŠ›",
                                placeholder="ã¾ãŸã¯ã€ä¸Šã®ãƒœã‚¿ãƒ³ã‹ã‚‰è³ªå•ã‚’é¸æŠã€‚Enterã§é€ä¿¡",
                                lines=1,
                                max_lines=1
                            )
                            submit_btn = gr.Button("é€ä¿¡", variant="primary")

                            # ãƒ¢ãƒ‡ãƒ«é¸æŠ
                            model_dropdown = gr.Dropdown(
                                label="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«",
                                choices=[],
                                value=None,
                                interactive=True
                            )
                            load_model_btn = gr.Button("ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰")
                            model_status = gr.Textbox(
                                label="ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹",
                                value="ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰",
                                interactive=False
                            )

                # ã‚¿ãƒ–2: ãƒ¢ãƒ‡ãƒ«ç®¡ç†
                with gr.Tab("ãƒ¢ãƒ‡ãƒ«ç®¡ç†"):
                    gr.Markdown("### ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«")
                    refresh_models_btn = gr.Button("ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’æ›´æ–°")
                    local_models_display = gr.DataFrame(
                        headers=["ãƒ¢ãƒ‡ãƒ«å", "ãƒ‘ã‚¹", "ã‚µã‚¤ã‚º"],
                        datatype=["str", "str", "str"],
                        label="ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«"
                    )

                    gr.Markdown("### ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                    with gr.Row():
                        with gr.Column():
                            preset_dropdown = gr.Dropdown(
                                label="ãƒ—ãƒªã‚»ãƒƒãƒˆ",
                                choices=list(self.model_presets.keys()),
                                value=None
                            )
                            repo_id_input = gr.Textbox(
                                label="Repository ID",
                                placeholder="Qwen/Qwen2-VL-7B-Instruct",
                                value=""
                            )
                            local_name_input = gr.Textbox(
                                label="ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å",
                                placeholder="qwen2-vl-7b",
                                value=""
                            )
                            download_btn = gr.Button("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹", variant="primary")

                        with gr.Column():
                            preset_info = gr.Markdown("ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸æŠã™ã‚‹ã¨è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™")
                            download_status = gr.Textbox(
                                label="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹",
                                value="",
                                interactive=False,
                                lines=5
                            )

                # ã‚¿ãƒ–3: è¨­å®š
                with gr.Tab("è¨­å®š"):
                    with gr.Row():
                        with gr.Column():
                            temperature_slider = gr.Slider(
                                label="Temperature",
                                info="ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’åˆ¶å¾¡ï¼ˆä½ã„å€¤=æ­£ç¢ºã€é«˜ã„å€¤=å‰µé€ çš„ï¼‰ã€‚ç”»åƒåˆ†æã§ã¯0.1ï½0.3ã‚’æ¨å¥¨",
                                minimum=0.0,
                                maximum=2.0,
                                value=initial_temperature,
                                step=0.1
                            )
                            max_tokens_slider = gr.Slider(
                                label="Max Tokens",
                                info="ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆæ–‡ç« ã®é•·ã•ï¼‰",
                                minimum=64,
                                maximum=2048,
                                value=initial_max_tokens,
                                step=64
                            )
                            top_p_slider = gr.Slider(
                                label="Top P",
                                info="èªå½™ã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡ã€‚0.9å‰å¾Œã‚’æ¨å¥¨",
                                minimum=0.0,
                                maximum=1.0,
                                value=initial_top_p,
                                step=0.05
                            )

            # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            # ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆchangeã‚¤ãƒ™ãƒ³ãƒˆã§å‡¦ç†ï¼‰
            image_display.change(
                fn=self.on_image_upload,
                inputs=[image_display],
                outputs=[prompt_display, negative_prompt_display, settings_display]
            )

            # ãƒãƒ£ãƒƒãƒˆ
            submit_btn.click(
                fn=self.chat_with_image,
                inputs=[user_input, chatbot, temperature_slider, max_tokens_slider],
                outputs=[chatbot, user_input, context_info, model_status]
            )

            # Enterã‚­ãƒ¼ã§ã‚‚é€ä¿¡
            user_input.submit(
                fn=self.chat_with_image,
                inputs=[user_input, chatbot, temperature_slider, max_tokens_slider],
                outputs=[chatbot, user_input, context_info, model_status]
            )

            # è³ªå•ãƒ—ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
            preset_btn_1.click(
                fn=self.preset_question_1,
                inputs=[chatbot, temperature_slider, max_tokens_slider],
                outputs=[chatbot, user_input, context_info, model_status]
            )

            preset_btn_2.click(
                fn=self.preset_question_2,
                inputs=[chatbot, temperature_slider, max_tokens_slider],
                outputs=[chatbot, user_input, context_info, model_status]
            )

            preset_btn_3.click(
                fn=self.preset_question_3,
                inputs=[chatbot, temperature_slider, max_tokens_slider],
                outputs=[chatbot, user_input, context_info, model_status]
            )

            preset_btn_4.click(
                fn=self.preset_question_4,
                inputs=[chatbot, temperature_slider, max_tokens_slider],
                outputs=[chatbot, user_input, context_info, model_status]
            )

            clear_btn.click(
                fn=lambda: [],
                outputs=[chatbot]
            )

            # ãƒ¢ãƒ‡ãƒ«ç®¡ç†
            refresh_models_btn.click(
                fn=self.refresh_local_models,
                outputs=[local_models_display, model_dropdown]
            )

            # ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã®å¤‰æ›´æ™‚ã«é¸æŠã‚’ä¿å­˜
            def save_selected_model(path):
                self.selected_model_path = path
                self.save_last_model_path(path) if path else None

            model_dropdown.change(
                fn=save_selected_model,
                inputs=[model_dropdown],
                outputs=[]
            )

            load_model_btn.click(
                fn=self.load_vlm_model,
                inputs=[model_dropdown],
                outputs=[model_status, context_info]
            )

            preset_dropdown.change(
                fn=self.update_preset_info,
                inputs=[preset_dropdown],
                outputs=[preset_info, repo_id_input, local_name_input]
            )

            download_btn.click(
                fn=self.download_model,
                inputs=[repo_id_input, local_name_input],
                outputs=[download_status]
            )

            # æ¨è«–è¨­å®šã®å¤‰æ›´æ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°
            def on_settings_change(temp, tokens, top_p):
                self.save_inference_settings(temp, tokens, top_p)

            temperature_slider.change(
                fn=on_settings_change,
                inputs=[temperature_slider, max_tokens_slider, top_p_slider],
                outputs=[]
            )
            max_tokens_slider.change(
                fn=on_settings_change,
                inputs=[temperature_slider, max_tokens_slider, top_p_slider],
                outputs=[]
            )
            top_p_slider.change(
                fn=on_settings_change,
                inputs=[temperature_slider, max_tokens_slider, top_p_slider],
                outputs=[]
            )

            # åˆæœŸãƒ­ãƒ¼ãƒ‰
            interface.load(
                fn=self.refresh_local_models,
                outputs=[local_models_display, model_dropdown]
            )

        return interface

    def on_image_upload(self, image_path: str) -> Tuple:
        """ç”»åƒãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã¨ãã®å‡¦ç†"""
        try:
            # ç”»åƒãƒ‘ã‚¹ãŒNoneã¾ãŸã¯ç©ºã®å ´åˆã¯ã‚¯ãƒªã‚¢
            if not image_path:
                self.current_image_path = None
                self.current_metadata = None
                return "", "", "{}"

            # ç”»åƒãƒ‘ã‚¹ã‚’ä¿å­˜
            self.current_image_path = image_path

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            self.current_metadata = ImageParser.extract_metadata(image_path)

            # Settingsã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
            settings_json = json.dumps(self.current_metadata['settings'], indent=2, ensure_ascii=False)

            return (
                self.current_metadata['prompt'],
                self.current_metadata['negative_prompt'],
                settings_json
            )
        except Exception as e:
            print(f"ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã‚‚çŠ¶æ…‹ã‚’ã‚¯ãƒªã‚¢
            self.current_image_path = None
            self.current_metadata = None
            return "ç”»åƒã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãƒ‰ãƒ­ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚", "", "{}"

    def preset_question_1(self, history: List, temperature: float, max_tokens: int):
        """ãƒ—ãƒªã‚»ãƒƒãƒˆè³ªå•1: ã“ã®ç”»åƒã«ã¤ã„ã¦èª¬æ˜"""
        for result in self.chat_with_image("ã“ã®ç”»åƒã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„", history, temperature, max_tokens):
            yield result

    def preset_question_2(self, history: List, temperature: float, max_tokens: int):
        """ãƒ—ãƒªã‚»ãƒƒãƒˆè³ªå•2: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã®ä¸€è‡´ç¢ºèª"""
        for result in self.chat_with_image("ã“ã®ç”»åƒã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ä¸€è‡´ã—ã¦ã„ã¾ã™ã‹?", history, temperature, max_tokens):
            yield result

    def preset_question_3(self, history: List, temperature: float, max_tokens: int):
        """ãƒ—ãƒªã‚»ãƒƒãƒˆè³ªå•3: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹å–„æ¡ˆ"""
        for result in self.chat_with_image("æ”¹å–„ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›¸ã„ã¦ãã ã•ã„", history, temperature, max_tokens):
            yield result

    def preset_question_4(self, history: List, temperature: float, max_tokens: int):
        """ãƒ—ãƒªã‚»ãƒƒãƒˆè³ªå•4: è©³ç´°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆææ¡ˆ"""
        for result in self.chat_with_image("ã‚ˆã‚Šè©³ç´°ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ææ¡ˆã—ã¦ãã ã•ã„", history, temperature, max_tokens):
            yield result

    def _get_model_status(self) -> str:
        """ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã‚’å–å¾—"""
        if self.current_vlm is None:
            return "ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰"
        if self.selected_model_path:
            return f"âœ“ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿: {Path(self.selected_model_path).name}"
        return "ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿"

    def chat_with_image(
        self,
        message: str,
        history: List,
        temperature: float,
        max_tokens: int
    ):
        """ç”»åƒã«ã¤ã„ã¦è³ªå•ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰"""
        max_tokens_int = int(max_tokens)

        if not message:
            yield history, "", self._get_context_info(history), self._get_model_status()
            return

        # ãƒ¢ãƒ‡ãƒ«ãŒæœªãƒ­ãƒ¼ãƒ‰ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆã¯è‡ªå‹•ãƒ­ãƒ¼ãƒ‰
        if self.current_vlm is None and self.selected_model_path:
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": "ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­..."})
            yield history, "", "<small style='color: gray;'>ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...</small>", "ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­..."

            # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
            status, context = self.load_vlm_model(self.selected_model_path)

            if "âœ“" not in status:
                # ãƒ­ãƒ¼ãƒ‰å¤±æ•—
                history[-1]["content"] = f"ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ\n{status}"
                yield history, "", "<small style='color: gray;'>--</small>", status
                return

            # ãƒ­ãƒ¼ãƒ‰æˆåŠŸã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‰Šé™¤ã—ã¦å†å®Ÿè¡Œ
            history.pop()
            history.pop()

        if self.current_vlm is None:
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": "ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"})
            yield history, "", "<small style='color: gray;'>--</small>", "ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
            return

        if not self.current_image_path or self.current_metadata is None:
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": "ã‚¨ãƒ©ãƒ¼: ç”»åƒãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“"})
            yield history, "", self._get_context_info(history), self._get_model_status()
            return

        # ç¾åœ¨ã®ç”»åƒãƒ‘ã‚¹
        prompt_text = self.current_metadata['prompt']

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…ˆã«è¿½åŠ 
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": ""})

        try:
            # VLMã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åˆ†æ
            response = ""
            for chunk in self.current_vlm.analyze_image_with_prompt_stream(
                image_path=self.current_image_path,
                prompt_text=prompt_text,
                question=message,
                temperature=temperature,
                max_tokens=max_tokens_int
            ):
                response += chunk
                history[-1]["content"] = response
                yield history, "", self._get_context_info(history), self._get_model_status()

        except Exception as e:
            history[-1]["content"] = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
            yield history, "", self._get_context_info(history), self._get_model_status()

    def _get_context_info(self, history: List) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å–å¾—ï¼ˆMarkdownå½¢å¼ï¼‰"""
        if self.current_vlm is None:
            return "<small style='color: gray;'>--</small>"

        # å±¥æ­´ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
        total_text = ""
        for msg in history:
            if isinstance(msg, dict) and "content" in msg:
                content = msg["content"]
                # contentãŒãƒªã‚¹ãƒˆã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã®ã¿æŠ½å‡º
                if isinstance(content, list):
                    for item in content:
                        if isinstance(item, dict) and item.get("type") == "text":
                            total_text += item.get("text", "") + "\n"
                        elif isinstance(item, str):
                            total_text += item + "\n"
                elif isinstance(content, str):
                    total_text += content + "\n"

        used_tokens = self.current_vlm.count_tokens(total_text)
        context_length = self.current_vlm.get_context_length()

        if context_length > 0:
            return f"<small style='color: gray;'>ğŸ“Š CONTEXT: {used_tokens:,} / {context_length:,}</small>"
        else:
            return f"<small style='color: gray;'>ğŸ“Š CONTEXT: {used_tokens:,}</small>"

    def refresh_local_models(self) -> Tuple:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’æ›´æ–°"""
        models = self.model_manager.list_local_models()

        # DataFrameãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        df_data = [[m['name'], m['path'], m['size']] for m in models]

        # ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ç”¨ã®é¸æŠè‚¢
        choices = [m['path'] for m in models]

        # å‰å›ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        last_model_path = self.load_last_model_path()

        # å‰å›ã®ãƒ¢ãƒ‡ãƒ«ãŒã¾ã å­˜åœ¨ã™ã‚‹å ´åˆã¯åˆæœŸå€¤ã«è¨­å®š
        if last_model_path and last_model_path in choices:
            self.selected_model_path = last_model_path
            return df_data, gr.Dropdown(choices=choices, value=last_model_path)

        return df_data, gr.Dropdown(choices=choices)

    def load_vlm_model(self, model_path: str) -> Tuple[str, str]:
        """VLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆGGUF/Transformersè‡ªå‹•åˆ¤å®šï¼‰"""
        if not model_path:
            return "ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“", "<small style='color: gray;'>--</small>"

        # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ä¿å­˜
        self.selected_model_path = model_path

        try:
            # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
            if self.current_vlm is not None:
                self.current_vlm.unload_model()

            # GGUFã‹Transformersã‹ã‚’åˆ¤å®š
            model_path_obj = Path(model_path)
            is_gguf = False

            if model_path_obj.is_file() and model_path_obj.suffix == '.gguf':
                is_gguf = True
            elif model_path_obj.is_dir():
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ç¢ºèª
                gguf_files = list(model_path_obj.glob('*.gguf'))
                if gguf_files:
                    is_gguf = True
                    model_path = str(gguf_files[0])  # æœ€åˆã®GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨

            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ãƒ­ãƒ¼ãƒ‰
            if is_gguf:
                # GGUFãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
                if not GGUF_AVAILABLE:
                    return "âœ— ã‚¨ãƒ©ãƒ¼: llama-cpp-pythonãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚GGUFãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€llama-cpp-pythonã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚", "<small style='color: gray;'>--</small>"

                gguf_config = self.config.get('gguf', {})
                self.current_vlm = VLMInterfaceGGUF(
                    model_path=model_path,
                    n_ctx=gguf_config.get('n_ctx', 4096),
                    n_gpu_layers=gguf_config.get('n_gpu_layers', -1),
                    verbose=gguf_config.get('verbose', False)
                )
                model_type_label = "GGUF"
            else:
                # Transformersãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
                self.current_vlm = VLMInterface(
                    model_path=model_path,
                    device=self.config['model']['device'],
                    dtype=self.config['model']['dtype']
                )
                model_type_label = "Transformers"

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’å–å¾—
            context_length = self.current_vlm.get_context_length()
            if context_length > 0:
                context_info = f"<small style='color: gray;'>ğŸ“Š CONTEXT: 0 / {context_length:,}</small>"
            else:
                context_info = "<small style='color: gray;'>ğŸ“Š CONTEXT: 0</small>"

            # æœ€å¾Œã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä¿å­˜
            self.save_last_model_path(model_path)

            return f"âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ [{model_type_label}]: {Path(model_path).name}", context_info

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âœ— ã‚¨ãƒ©ãƒ¼: {str(e)}\n\nè©³ç´°:\n{error_detail}", "<small style='color: gray;'>--</small>"

    def update_preset_info(self, preset_name: str) -> Tuple:
        """ãƒ—ãƒªã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¡¨ç¤º"""
        if not preset_name or preset_name not in self.model_presets:
            return "ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸æŠã™ã‚‹ã¨è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™", "", ""

        preset = self.model_presets[preset_name]

        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’å–å¾—ï¼ˆGGUFã‹ã©ã†ã‹ï¼‰
        model_type = preset.get('model_type', 'transformers')
        model_type_label = "GGUF" if model_type == 'gguf' else "Transformers"

        info_md = f"""
### {preset_name}

**ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—**: {model_type_label}
**èª¬æ˜**: {preset['description']}
**æ¨å¥¨ç”¨é€”**: {preset['recommended_for']}
**Repository ID**: `{preset['repo_id']}`
"""

        # GGUFã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚‚è¡¨ç¤º
        if 'filename' in preset:
            info_md += f"\n**ãƒ•ã‚¡ã‚¤ãƒ«å**: `{preset['filename']}`"

        return info_md, preset['repo_id'], preset['local_name']

    def save_last_model_path(self, model_path: str):
        """æœ€å¾Œã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’ä¿å­˜ï¼ˆsettingså«ã‚€ï¼‰"""
        try:
            # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            data = {}
            if self.last_model_cache_file.exists():
                try:
                    data = json.loads(self.last_model_cache_file.read_text(encoding='utf-8'))
                except:
                    pass

            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’æ›´æ–°
            data["last_model"] = model_path

            self.last_model_cache_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
        except Exception as e:
            print(f"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def save_inference_settings(self, temperature: float, max_tokens: int, top_p: float):
        """æ¨è«–è¨­å®šã‚’ä¿å­˜"""
        try:
            # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            data = {}
            if self.last_model_cache_file.exists():
                try:
                    data = json.loads(self.last_model_cache_file.read_text(encoding='utf-8'))
                except:
                    pass

            # è¨­å®šã‚’æ›´æ–°
            data["inference_settings"] = {
                "temperature": temperature,
                "max_tokens": int(max_tokens),
                "top_p": top_p
            }

            self.last_model_cache_file.write_text(
                json.dumps(data, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
        except Exception as e:
            print(f"è­¦å‘Š: æ¨è«–è¨­å®šã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def load_last_model_path(self) -> Optional[str]:
        """æœ€å¾Œã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.last_model_cache_file.exists():
                data = json.loads(self.last_model_cache_file.read_text(encoding='utf-8'))
                return data.get("last_model")
        except Exception as e:
            print(f"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

    def load_inference_settings(self) -> dict:
        """æ¨è«–è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        try:
            if self.last_model_cache_file.exists():
                data = json.loads(self.last_model_cache_file.read_text(encoding='utf-8'))
                return data.get("inference_settings", {})
        except Exception as e:
            print(f"è­¦å‘Š: æ¨è«–è¨­å®šã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return {}

    def download_model(self, repo_id: str, local_name: str) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆGGUFå¯¾å¿œï¼‰"""
        if not repo_id:
            return "ã‚¨ãƒ©ãƒ¼: Repository IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"

        try:
            # é¸æŠã•ã‚ŒãŸãƒ—ãƒªã‚»ãƒƒãƒˆã‹ã‚‰æƒ…å ±ã‚’å–å¾—
            filename = None
            for preset_name, preset in self.model_presets.items():
                if preset['repo_id'] == repo_id and preset['local_name'] == local_name:
                    filename = preset.get('filename', None)
                    break

            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
            downloaded_path = self.model_manager.download_model(
                repo_id=repo_id,
                local_name=local_name if local_name else None,
                filename=filename
            )

            if filename:
                return f"âœ“ GGUFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†\nä¿å­˜å…ˆ: {downloaded_path}"
            else:
                return f"âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†\nä¿å­˜å…ˆ: {downloaded_path}"

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—\nã‚¨ãƒ©ãƒ¼: {str(e)}\n\nè©³ç´°:\n{error_detail}"
