"""
Gradio UIå®Ÿè£…
"""
import gradio as gr
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from PIL import Image

from src.core.image_parser import ImageParser
from src.core.model_manager import ModelManager
from src.core.vlm_interface import VLMInterface
from src.utils.image_utils import get_image_files
from src.utils.config_loader import ConfigLoader

# GGUFå¯¾å¿œï¼ˆllama-cpp-pythonãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ï¼‰
try:
    from src.core.vlm_interface_gguf import VLMInterfaceGGUF
    GGUF_AVAILABLE = True
except ImportError:
    GGUF_AVAILABLE = False
    print("Warning: llama-cpp-python is not installed. GGUF models will not be available.")


class PromptAnalyzerUI:
    """ãƒ¡ã‚¤ãƒ³UIã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: Dict):
        """
        Args:
            config: settings.yamlã‹ã‚‰èª­ã¿è¾¼ã‚“ã è¨­å®š
        """
        self.config = config
        self.model_manager = ModelManager(config['paths']['models_dir'])
        self.current_vlm: Optional[VLMInterface] = None
        self.image_list: List[Path] = []
        self.current_index: int = 0
        self.current_metadata: Optional[Dict] = None

        # ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
        config_loader = ConfigLoader()
        self.model_presets = config_loader.load_model_presets()

    def create_interface(self) -> gr.Blocks:
        """
        Gradio UIã‚’æ§‹ç¯‰

        UIæ§‹æˆ:
        - ã‚¿ãƒ–1: ç”»åƒåˆ†æ
        - ã‚¿ãƒ–2: ãƒ¢ãƒ‡ãƒ«ç®¡ç†
        - ã‚¿ãƒ–3: è¨­å®š
        """
        with gr.Blocks(title="SD Prompt Analyzer") as interface:
            gr.Markdown("# SD Prompt Analyzer")
            gr.Markdown("Stable Diffusionç”»åƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ†æã™ã‚‹ãƒ„ãƒ¼ãƒ«")

            with gr.Tabs():
                # ã‚¿ãƒ–1: ç”»åƒåˆ†æ
                with gr.Tab("ç”»åƒåˆ†æ"):
                    with gr.Row():
                        # å·¦å´: ç”»åƒè¡¨ç¤º
                        with gr.Column(scale=1):
                            image_display = gr.Image(label="ç”»åƒ", type="pil", height=400)

                            with gr.Row():
                                prev_btn = gr.Button("â† å‰ã¸", size="sm")
                                next_btn = gr.Button("æ¬¡ã¸ â†’", size="sm")

                            folder_path = gr.Textbox(
                                label="ç”»åƒãƒ•ã‚©ãƒ«ãƒ€",
                                value=self.config['paths']['image_folder'],
                                placeholder="./data/sd_outputs"
                            )
                            load_folder_btn = gr.Button("ãƒ•ã‚©ãƒ«ãƒ€ã‚’èª­ã¿è¾¼ã¿", variant="primary")

                            image_counter = gr.Textbox(
                                label="ç”»åƒç•ªå·",
                                value="0 / 0",
                                interactive=False
                            )

                            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±è¡¨ç¤º
                            with gr.Accordion("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±", open=True):
                                prompt_display = gr.Textbox(
                                    label="Prompt",
                                    lines=3,
                                    interactive=False
                                )
                                negative_prompt_display = gr.Textbox(
                                    label="Negative Prompt",
                                    lines=2,
                                    interactive=False
                                )
                                settings_display = gr.JSON(label="Settings", value={})

                        # å³å´: ãƒãƒ£ãƒƒãƒˆ
                        with gr.Column(scale=1):
                            chatbot = gr.Chatbot(label="AIåˆ†æ", height=500)
                            context_info = gr.Markdown(
                                value="<small style='color: gray;'>--</small>",
                                elem_id="context-info"
                            )
                            user_input = gr.Textbox(
                                label="è³ªå•ã‚’å…¥åŠ›",
                                placeholder="ã“ã®ç”»åƒã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ä¸€è‡´ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ",
                                lines=2
                            )
                            submit_btn = gr.Button("é€ä¿¡", variant="primary")
                            clear_btn = gr.Button("ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚¯ãƒªã‚¢")

                            # ãƒ¢ãƒ‡ãƒ«é¸æŠ
                            model_dropdown = gr.Dropdown(
                                label="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«",
                                choices=[],
                                value=None,
                                interactive=True
                            )
                            load_model_btn = gr.Button("ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰")
                            model_status = gr.Textbox(
                                label="ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹",
                                value="ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰",
                                interactive=False
                            )

                # ã‚¿ãƒ–2: ãƒ¢ãƒ‡ãƒ«ç®¡ç†
                with gr.Tab("ãƒ¢ãƒ‡ãƒ«ç®¡ç†"):
                    gr.Markdown("### ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«")
                    refresh_models_btn = gr.Button("ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’æ›´æ–°")
                    local_models_display = gr.DataFrame(
                        headers=["ãƒ¢ãƒ‡ãƒ«å", "ãƒ‘ã‚¹", "ã‚µã‚¤ã‚º"],
                        datatype=["str", "str", "str"],
                        label="ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«"
                    )

                    gr.Markdown("### ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                    with gr.Row():
                        with gr.Column():
                            preset_dropdown = gr.Dropdown(
                                label="ãƒ—ãƒªã‚»ãƒƒãƒˆ",
                                choices=list(self.model_presets.keys()),
                                value=None
                            )
                            repo_id_input = gr.Textbox(
                                label="Repository ID",
                                placeholder="Qwen/Qwen2-VL-7B-Instruct",
                                value=""
                            )
                            local_name_input = gr.Textbox(
                                label="ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜å",
                                placeholder="qwen2-vl-7b",
                                value=""
                            )
                            download_btn = gr.Button("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹", variant="primary")

                        with gr.Column():
                            preset_info = gr.Markdown("ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸æŠã™ã‚‹ã¨è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™")
                            download_status = gr.Textbox(
                                label="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹",
                                value="",
                                interactive=False,
                                lines=5
                            )

                # ã‚¿ãƒ–3: è¨­å®š
                with gr.Tab("è¨­å®š"):
                    with gr.Row():
                        with gr.Column():
                            temperature_slider = gr.Slider(
                                label="Temperature",
                                minimum=0.0,
                                maximum=2.0,
                                value=self.config['inference']['temperature'],
                                step=0.1
                            )
                            max_tokens_slider = gr.Slider(
                                label="Max Tokens",
                                minimum=64,
                                maximum=2048,
                                value=self.config['inference']['max_tokens'],
                                step=64
                            )
                            top_p_slider = gr.Slider(
                                label="Top P",
                                minimum=0.0,
                                maximum=1.0,
                                value=self.config['inference']['top_p'],
                                step=0.05
                            )

            # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            # ãƒ•ã‚©ãƒ«ãƒ€èª­ã¿è¾¼ã¿
            load_folder_btn.click(
                fn=self.load_image_folder,
                inputs=[folder_path],
                outputs=[image_display, image_counter, prompt_display,
                         negative_prompt_display, settings_display]
            )

            # ç”»åƒãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            next_btn.click(
                fn=self.next_image,
                outputs=[image_display, image_counter, prompt_display,
                         negative_prompt_display, settings_display]
            )

            prev_btn.click(
                fn=self.prev_image,
                outputs=[image_display, image_counter, prompt_display,
                         negative_prompt_display, settings_display]
            )

            # ãƒãƒ£ãƒƒãƒˆ
            submit_btn.click(
                fn=self.chat_with_image,
                inputs=[user_input, chatbot, temperature_slider, max_tokens_slider],
                outputs=[chatbot, user_input, context_info]
            )

            clear_btn.click(
                fn=lambda: [],
                outputs=[chatbot]
            )

            # ãƒ¢ãƒ‡ãƒ«ç®¡ç†
            refresh_models_btn.click(
                fn=self.refresh_local_models,
                outputs=[local_models_display, model_dropdown]
            )

            load_model_btn.click(
                fn=self.load_vlm_model,
                inputs=[model_dropdown],
                outputs=[model_status, context_info]
            )

            preset_dropdown.change(
                fn=self.update_preset_info,
                inputs=[preset_dropdown],
                outputs=[preset_info, repo_id_input, local_name_input]
            )

            download_btn.click(
                fn=self.download_model,
                inputs=[repo_id_input, local_name_input],
                outputs=[download_status]
            )

            # åˆæœŸãƒ­ãƒ¼ãƒ‰
            interface.load(
                fn=self.refresh_local_models,
                outputs=[local_models_display, model_dropdown]
            )

        return interface

    def load_image_folder(self, folder_path: str) -> Tuple:
        """ç”»åƒãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¹ã‚­ãƒ£ãƒ³"""
        self.image_list = get_image_files(folder_path)
        self.current_index = 0

        if not self.image_list:
            return None, "0 / 0", "", "", {}

        return self._get_current_image_data()

    def next_image(self) -> Tuple:
        """æ¬¡ã®ç”»åƒã«ç§»å‹•"""
        if not self.image_list:
            return None, "0 / 0", "", "", {}

        self.current_index = (self.current_index + 1) % len(self.image_list)
        return self._get_current_image_data()

    def prev_image(self) -> Tuple:
        """å‰ã®ç”»åƒã«ç§»å‹•"""
        if not self.image_list:
            return None, "0 / 0", "", "", {}

        self.current_index = (self.current_index - 1) % len(self.image_list)
        return self._get_current_image_data()

    def _get_current_image_data(self) -> Tuple:
        """ç¾åœ¨ã®ç”»åƒã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if not self.image_list:
            return None, "0 / 0", "", "", {}

        current_image_path = self.image_list[self.current_index]

        # ç”»åƒã‚’èª­ã¿è¾¼ã¿
        image = Image.open(current_image_path)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        self.current_metadata = ImageParser.extract_metadata(str(current_image_path))

        # ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        counter = f"{self.current_index + 1} / {len(self.image_list)}"

        return (
            image,
            counter,
            self.current_metadata['prompt'],
            self.current_metadata['negative_prompt'],
            self.current_metadata['settings']
        )

    def chat_with_image(
        self,
        message: str,
        history: List,
        temperature: float,
        max_tokens: int
    ):
        """ç”»åƒã«ã¤ã„ã¦è³ªå•ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰"""
        max_tokens_int = int(max_tokens)

        if not message:
            yield history, "", self._get_context_info(history)
            return

        if self.current_vlm is None:
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": "ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“"})
            yield history, "", "<small style='color: gray;'>--</small>"
            return

        if not self.image_list or self.current_metadata is None:
            history.append({"role": "user", "content": message})
            history.append({"role": "assistant", "content": "ã‚¨ãƒ©ãƒ¼: ç”»åƒãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“"})
            yield history, "", self._get_context_info(history)
            return

        # ç¾åœ¨ã®ç”»åƒãƒ‘ã‚¹
        current_image_path = str(self.image_list[self.current_index])
        prompt_text = self.current_metadata['prompt']

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…ˆã«è¿½åŠ 
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": ""})

        try:
            # VLMã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åˆ†æ
            response = ""
            for chunk in self.current_vlm.analyze_image_with_prompt_stream(
                image_path=current_image_path,
                prompt_text=prompt_text,
                question=message,
                temperature=temperature,
                max_tokens=max_tokens_int
            ):
                response += chunk
                history[-1]["content"] = response
                yield history, "", self._get_context_info(history)

        except Exception as e:
            history[-1]["content"] = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
            yield history, "", self._get_context_info(history)

    def _get_context_info(self, history: List) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å–å¾—ï¼ˆMarkdownå½¢å¼ï¼‰"""
        if self.current_vlm is None:
            return "<small style='color: gray;'>--</small>"

        # å±¥æ­´ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—
        total_text = ""
        for msg in history:
            if isinstance(msg, dict) and "content" in msg:
                content = msg["content"]
                # contentãŒãƒªã‚¹ãƒˆã®å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã®ã¿æŠ½å‡º
                if isinstance(content, list):
                    for item in content:
                        if isinstance(item, dict) and item.get("type") == "text":
                            total_text += item.get("text", "") + "\n"
                        elif isinstance(item, str):
                            total_text += item + "\n"
                elif isinstance(content, str):
                    total_text += content + "\n"

        used_tokens = self.current_vlm.count_tokens(total_text)
        context_length = self.current_vlm.get_context_length()

        if context_length > 0:
            return f"<small style='color: gray;'>ğŸ“Š CONTEXT: {used_tokens:,} / {context_length:,}</small>"
        else:
            return f"<small style='color: gray;'>ğŸ“Š CONTEXT: {used_tokens:,}</small>"

    def refresh_local_models(self) -> Tuple:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’æ›´æ–°"""
        models = self.model_manager.list_local_models()

        # DataFrameãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        df_data = [[m['name'], m['path'], m['size']] for m in models]

        # ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ç”¨ã®é¸æŠè‚¢
        choices = [m['path'] for m in models]

        return df_data, gr.Dropdown(choices=choices)

    def load_vlm_model(self, model_path: str) -> Tuple[str, str]:
        """VLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆGGUF/Transformersè‡ªå‹•åˆ¤å®šï¼‰"""
        if not model_path:
            return "ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“", "<small style='color: gray;'>--</small>"

        try:
            # æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰
            if self.current_vlm is not None:
                self.current_vlm.unload_model()

            # GGUFã‹Transformersã‹ã‚’åˆ¤å®š
            model_path_obj = Path(model_path)
            is_gguf = False

            if model_path_obj.is_file() and model_path_obj.suffix == '.gguf':
                is_gguf = True
            elif model_path_obj.is_dir():
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ç¢ºèª
                gguf_files = list(model_path_obj.glob('*.gguf'))
                if gguf_files:
                    is_gguf = True
                    model_path = str(gguf_files[0])  # æœ€åˆã®GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨

            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ãƒ­ãƒ¼ãƒ‰
            if is_gguf:
                # GGUFãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
                if not GGUF_AVAILABLE:
                    return "âœ— ã‚¨ãƒ©ãƒ¼: llama-cpp-pythonãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚GGUFãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€llama-cpp-pythonã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚", "<small style='color: gray;'>--</small>"

                gguf_config = self.config.get('gguf', {})
                self.current_vlm = VLMInterfaceGGUF(
                    model_path=model_path,
                    n_ctx=gguf_config.get('n_ctx', 4096),
                    n_gpu_layers=gguf_config.get('n_gpu_layers', -1),
                    verbose=gguf_config.get('verbose', False)
                )
                model_type_label = "GGUF"
            else:
                # Transformersãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
                self.current_vlm = VLMInterface(
                    model_path=model_path,
                    device=self.config['model']['device'],
                    dtype=self.config['model']['dtype']
                )
                model_type_label = "Transformers"

            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’å–å¾—
            context_length = self.current_vlm.get_context_length()
            if context_length > 0:
                context_info = f"<small style='color: gray;'>ğŸ“Š CONTEXT: 0 / {context_length:,}</small>"
            else:
                context_info = "<small style='color: gray;'>ğŸ“Š CONTEXT: 0</small>"

            return f"âœ“ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ [{model_type_label}]: {Path(model_path).name}", context_info

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âœ— ã‚¨ãƒ©ãƒ¼: {str(e)}\n\nè©³ç´°:\n{error_detail}", "<small style='color: gray;'>--</small>"

    def update_preset_info(self, preset_name: str) -> Tuple:
        """ãƒ—ãƒªã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¡¨ç¤º"""
        if not preset_name or preset_name not in self.model_presets:
            return "ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’é¸æŠã™ã‚‹ã¨è©³ç´°ãŒè¡¨ç¤ºã•ã‚Œã¾ã™", "", ""

        preset = self.model_presets[preset_name]

        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’å–å¾—ï¼ˆGGUFã‹ã©ã†ã‹ï¼‰
        model_type = preset.get('model_type', 'transformers')
        model_type_label = "GGUF" if model_type == 'gguf' else "Transformers"

        info_md = f"""
### {preset_name}

**ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—**: {model_type_label}
**èª¬æ˜**: {preset['description']}
**æ¨å¥¨ç”¨é€”**: {preset['recommended_for']}
**Repository ID**: `{preset['repo_id']}`
"""

        # GGUFã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚‚è¡¨ç¤º
        if 'filename' in preset:
            info_md += f"\n**ãƒ•ã‚¡ã‚¤ãƒ«å**: `{preset['filename']}`"

        return info_md, preset['repo_id'], preset['local_name']

    def download_model(self, repo_id: str, local_name: str) -> str:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆGGUFå¯¾å¿œï¼‰"""
        if not repo_id:
            return "ã‚¨ãƒ©ãƒ¼: Repository IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"

        try:
            # é¸æŠã•ã‚ŒãŸãƒ—ãƒªã‚»ãƒƒãƒˆã‹ã‚‰æƒ…å ±ã‚’å–å¾—
            filename = None
            for preset_name, preset in self.model_presets.items():
                if preset['repo_id'] == repo_id and preset['local_name'] == local_name:
                    filename = preset.get('filename', None)
                    break

            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
            downloaded_path = self.model_manager.download_model(
                repo_id=repo_id,
                local_name=local_name if local_name else None,
                filename=filename
            )

            if filename:
                return f"âœ“ GGUFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†\nä¿å­˜å…ˆ: {downloaded_path}"
            else:
                return f"âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†\nä¿å­˜å…ˆ: {downloaded_path}"

        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return f"âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—\nã‚¨ãƒ©ãƒ¼: {str(e)}\n\nè©³ç´°:\n{error_detail}"
